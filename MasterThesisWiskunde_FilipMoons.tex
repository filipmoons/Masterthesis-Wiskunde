% $Id: $
\documentclass[a4paper,11pt]{report}
\usepackage{a4wide}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{amsfonts}
% The following makes latex use nicer postscript fonts.
\usepackage{times}
\usepackage{subcaption}
\usepackage[english]{babel}
\usepackage{tikz}
%\usepackage[colorlinks,urlcolor=blue,linkcolor=blue]{hyperref}
\pagestyle{headings}
\newcommand{\upuparrow}{\mathrel{\reflectbox{\rotatebox[origin=c]{90}{$\twoheadrightarrow$}}}}
\newcommand{\downdownarrow}{\mathrel{\reflectbox{\rotatebox[origin=c]{90}{$\twoheadleftarrow$}}}}
\usepackage{vubtitlepage}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage[geometry]{ifsym}
%\usepackage[font=small,format=plain,labelfont=bf,up,textfont=it,up]{caption}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\author{Filip Moons}
\title{Tiny Compiler}
%\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{example}[theorem]{Example}
\let\oldexample\example
\renewcommand{\example}{\oldexample\normalfont}
\newtheorem{property}[theorem]{Property}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{examples}[theorem]{Examples}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{notation}[theorem]{Notation}

\setcounter{tocdepth}{5}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\HQ}{{\mathbb H}}
\renewcommand{\P}{{\mathbb P}}
\newcommand{\E}{{\mathbb E}}
\newcommand{\cost}{\text{cost}}
\newcommand{\Nash}{\text{Nash}}
\newcommand{\nash}{\text{nash}}
\newcommand{\opt}{\text{opt}}
\newcommand{\LFP}{\text{LFP}}
\renewcommand{\int}{\text{int}}
\newcommand{\graf}{\mathscr{G}}
\newcommand{\grafeen}{\mathscr{H}}
%\newenvironment{proof}{\noindent{\bf Bewijs.}}{{\hfill $ \ Box $}\vskip 4mm}

\promotortitle{Promotor}
\promotor{Prof. Dr. P. Cara}
\advisors{}
\author{Filip Moons}
\title{Similarity on Combinatorial Structures}
\advisortitle{}
\addto\captionsenglish{\renewcommand*\abstractname{Abstract for non-mathematicians}}
\date{MEI 2006}
\faculty{Faculty of Science}
\advisortitle{}
\department{Master in Mathematics - specialization: Education}
\reason{Graduation thesis submitted in partial fulfillment of the requirements
for\\ the degree of Master of Science in Mathematics - specialization: Education}

\date{June 2015}


\begin{document}
% Then english TitlePage
\maketitlepage


\tableofcontents
\newpage
% \pagenumbering{arabic}
\chapter{Preliminaries and notations}


The Perron-Frobenius theorem states that a real square matrix with nonnegative entries has a unique largest real eigenvalue with an eigenvector 
that has only positive entries. The theorem was proved by Oskar 
Perron (1880-1975) in 1907 for strictly positive entries and extended by 
Ferdinand Georg Frobenius (1849-1917) to irreducible matrices with nonnegative 
entries. 

\section{Some families of matrices}
In this section, we first introduce different kinds of matrices. We start with permutation matrices and their uses. 
With permutation matrices, we can introduce irreducible matrices. Also nonnegative and primitive square matrices are presented. After defining 
those, we look at the Perron-Frobenius theorem. 
\subsection{Permutation matrices}
\begin{definition}
  Given a permutation $\pi$ of $n$ elements:
  $$\pi: \{1,\ldots,n\} \to \{1,\ldots,n\},$$
  with:
    $$\pi = \begin{pmatrix} 1 & 2 & \cdots & n \\ \pi(1) & \pi(2) & \cdots & \pi(n) \end{pmatrix} $$
  the associated \textbf{permutation matrix} $P_\pi$ is the $n\times n$-matrix 
  obtained by permuting the rows of the identity matrix $I_n$ according to $\pi$. 
  So:
 $$P_\pi = \begin{bmatrix} 
\mathbf{e}_{\pi(1)}  \\
\mathbf{e}_{\pi(2)}  \\
\vdots  \\
\mathbf{e}_{\pi(n)}  
\end{bmatrix}.$$
 where $\mathbf{e}_{j}$ is the $j$-th row of $I_n$.
 \end{definition}
\begin{example}
  The permutation matrix $P_\pi$ corresponding to the permutation  $$\pi = \begin{pmatrix} 1 & 2 & 3 & 4 \\ 1 & 4 & 2 & 3 \end{pmatrix} $$
  is:
  $$P_\pi = \begin{bmatrix} 
1 & 0 & 0 & 0  \\
0 & 0 & 0 & 1  \\
0 & 1 & 0 & 0  \\
0 & 0 & 1 & 0  
\end{bmatrix}.$$
Note that $p_{ij} = 1$ if and only if $\pi(i) = j$. 
\end{example}

\begin{property}\label{permutatie}
A permutation matrix $P$ satisfies:
  $$PP^T = I_n,$$
  where $P^T$ is the transpose and $I_n$ is the identity matrix.
\end{property}
\begin{proof}
By direct computation, we get:
$$(PP^T)_{ij} = \sum^n_{k=1}P_{ik}P^T_{kj} = \sum^n_{k=1}P_{ik}P_{jk}$$
Assume $i \not = j$. Then for each $k$, $P_{ik}P_{jk} = 0$ since there is only
one nonzero entry in the $k$-th row and $i \not = j$, $P_{ik}$ and $P_{jk}$ 
can't be both the nonzero entry. So, $(PP^T)_{ij} = 0$ when $i \not = j$.

When $i=j$, then there exists a $k' \in \{1,\ldots,n\}$ with $P_{ik'}P_{jk'} = 1$, 
since there is only one nonzero entry in the $k$-th row, this $k'$ is unique, 
which results in $\sum^n_{k=1}P_{ik}P_{jk} = (PP^T)_{ij} = 1$.

In other words,
$$(PP^T)_{ij} = \begin{cases} 1 &\mbox{if } i = j \\ 
0 & \mbox{otherwise} \end{cases},$$
this is exactly the formula for the entries of the identity matrix.
\end{proof}

\begin{corollary}
  The transpose of a permutation matrix $P$ is its inverse:
  $$P^T = P^{-1}.$$
  This can also more easily be concluded by the fact that a permutation matrix 
  is clearly an orthogonal matrix (a real $n \times n$-matrix with orthonormal 
  entries).
\end{corollary}
\subsection{Nonnegative and primitive matrices}
\begin{definition}
  Let $A$ and $B$ be two real $n\times r$-matrices. Then, $A \geq B$ (respectively $A > B$) if $a_{ij} \geq b_{ij}$ 
  (respectively $a_{ij}>b_{ij}$) for all $1 \leq i \leq n, 1 \leq j\leq r$. 
\end{definition}
\begin{definition}  A real $n\times r$-matrix $A$ is \textbf{nonnegative} if $A \geq 0$, with $0$ the $n\timesr$-null matrix.
\end{definition}
\begin{definition}  A real $n\times r$-matrix $A$ is \textbf{positive} if $A > 0$, with $0$ the $n\timesr$-null matrix.
\end{definition}

Since row vectors are $1 \times n$-matrices, we shall use the terms 
nonnegative and positive vector throughout. 

\begin{notation}\label{modulusmatrix}
  Let $B$ be an arbitrary complex $n\times r$-matrix, then $|B|$ denotes the 
  matrix with entries $|b_{ij}|$. This is not to be confused with the 
  determinant of a square matrix $B$, which we denote by $\det(B)$.
\end{notation}

\begin{definition}  A nonnegative square matrix $A$ is called \textbf{primitive} if there is a $k \in \N_0$ such that all entries of $A^k$ are positive.
 \end{definition}

\subsection{Irreducible nonnegative matrices}
In developing the Perron-Frobenius theory, we shall first establish a series of 
theorems and lemmas on nonnegative irredicuble square matrices. 
\begin{definition}\label{defreduciebel} A square matrix $A$ is called \textbf{reducible} if there is a permutation matrix $P$ such that 
 $$PAP^T = \begin{pmatrix}  B  & 0\\
 C  & D\\
\end{pmatrix} $$
where $B,$ and $D$ are square matrices, each of size at least one and $0$ is a zero matrix.
A square matrix $A$ is called \textbf{irreducible} if it is not reducible.
 \end{definition}
It follows immediately that a $1\times1$-matrix is always irreducible by 
definition. We now show a useful property to identify a reducible matrix.
\begin{property}\label{propertyreducible}
  Let $A$ be an $n\times n$-matrix with $n\geq 2$. Consider a nonempty, proper subset $S$ of $\{1,\ldots,n\}$ with 
  $a_{ij}=0$ for $i \in S$, 
  $j \not\in S$. Then A is  reducible.
\end{property}
\begin{proof}
  Let $S=\{i_1, i_2,\ldots, i_k\}$, where we assume, without loss of generality, 
  that $i_1 < i_2 < \cdots < i_{k-1} < i_k$. Let $S^c$ be the complement of $S$, consisting of the
  ordered set of elements $j_1 < j_3 < \cdots < j_{n-k}$. Consider the permutation $\sigma$ of
  $\{1,2,\ldots,n\}$ given by
  $$\sigma = \begin{pmatrix} 1 & 2 & \ldots & k & k + 1 & k+ 2 & \ldots & n \\ 
  i_1 & i_2 & \ldots & i_k & j_1 & j_2 & \ldots & j_{n-k} \end{pmatrix} $$

$\sigma$ can be represented by the permutation matrix $P_\sigma = (p_{ij})$, 
where $p_{rs} = 1$ if $\sigma(r) = s$. We prove that
 $$PAP^T = \begin{pmatrix}  B  & 0\\
 C  & D\\
\end{pmatrix} $$

where $B$ and $D$ are square matrices and $0$ is a $k \times (n-k)$ zero matrix. 
Consider row $c$ and column $d$, where $1 \leq c \leq k$ and $k + 1 \leq d \leq
n$:
\begin{eqnarray}\label{redenering1}
(PAP^T)_{cd} = \sum_i\sum_j p_{c i}a_{ij}p_{d j}.
\end{eqnarray}
It is enough to show that each term in the summation is zero. Suppose $p_{c i} =
p_{d j} = 1$. Thus $\sigma(c) = i$ and $\sigma(d)= j.$ Since $1 \leq c \leq
k$, then $i \in \{i_1, i_2,\ldots, i_k\}$; similarly, since $k+1 \leq d \leq n$, we have $j \in 
\{j_1,j_2,\ldots,j_{n-k}\}.$ By assumption, for such a pair $i, j$, we have $a_{ij}=0$. That completes
the proof. 
\end{proof}
We know prove some equivalent definitions for a nonnegative, irreducible square 
matrix.
\begin{theorem}\label{irredubieleegenschappen}
  Let $A\geq 0$ be a nonnegative $n\times n$-matrix. Then the following conditions are 
  equivalent:
  \begin{enumerate}
    \item[(1)] $A$ is irreducible.
    \item[(2)] $(I + A)^{n-1} > 0$
   \item[(3)] For any pair $(i,j)$, with $1 \leq i, j \leq n$, there is a 
   positive integer $k = k(i,j) \leq n$ such that $(A^k)_{ij} > 0$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  
  $(1) \Rightarrow (2)$: Let $\mathbf{x} \geq 0, \mathbf{x} \not = \mathbf{0}$ be an arbitrary vector in $\R^n$.
  If a coordinate of $\mathbf{x}$ is positive, the same coordinate is positive  in $\mathbf{x} + A\mathbf{x} = (I+A)\mathbf{x}$ 
  as well. We claim that $(I+A)\mathbf{x}$ has fewer zero coordinates than 
  $\mathbf{x}$ as long as $\mathbf{x}$ has a zero coordinate. If this claim is 
  not true, then the number of zero coordinates must be at least equal,
  this means that for each coordinate $j$ with $x_j = 0$ we would have that $x_j+ (A\mathbf{x})_j= 0$. Let $J = \{j: x_j > 0\}.$ For any $j \not \in J, r\in J$, we have 
  $(A\mathbf{x})_j = \sum_k a_{jk}x_k = 0$ and $x_r > 0$. 
  It must be that $a_{jr} = 0$. It follows from Property \ref{propertyreducible} that $A$ 
  is reducible, which is a contradiction and the claim is proved. Thus $(I+A)\mathbf{x}$ 
  has at most $n-2$ zero coordinates. Continuing in this manner we conclude that 
  $(I + A)^{n-1}\mathbf{x} > 0$. Let $\mathbf{x} = \mathbf{e}_i$, then the corresponding column of $(I+A)^{n-1}$ must be 
  positive. Thus (2) holds.
  
  $(2) \Rightarrow (3)$: We have $(I + A)^{n-1} > 0$, $A \geq 0$, so $A \not = 0$ 
  and 
  $$A(I+A)^{n-1} = \sum_{k=1}^n {n-1 \choose k-1} A^k > 0.$$
Thus for any $i, j$ at least one of the matrices $A, A^2,\ldots, A^n$ has its 
$(i,j)$-th element entry positive.

$(3) \Rightarrow (1)$: Suppose $A$ is reducible. Then for some permutation 
matrix $P$,
$$PAP^T = \begin{pmatrix}  B_1  & 0\\
 C_1  & D_1\\
\end{pmatrix} $$
where $B_1$ and $D_1$ are square matrices. Furthermore, we know from Property \ref{permutatie} that 
$PAP^TPAP^T = PA^2P^T$,
whence for some square matrices $B_2, C_2$ we have:
$$PA^2P^T = \begin{pmatrix}  B_2  & 0\\
 C_2  & D_2\\
\end{pmatrix} $$
More generally, for some matrix $C_t$ and square matrices $B_t$ and $D_t$,
$$PA^tP^T = \begin{pmatrix}  B_t  & 0\\
 C_t & D_t\\
\end{pmatrix} $$
Thus $(PA^tP^T)_{rs} = 0$ for $t = 1, 2,\ldots$ and for any $r,s$ 
corresponding to an entry of the zero submatrix in $PAP^T$.
Now, for $t = 1,\ldots,n:$
$$0 = (PA^tP^T)_{rs} = \sum_k\sum_l p_{r k}a_{kl}^{(t)}p_{s t}$$
By using the same reasoning as in \ref{redenering1}, choose $k,l$ so that $p_{r k}$ = $p_{s l} = 1$. Then $a^{(t)}_{kl} = 0$ for all $t$,
contradicting the hypothesis. This completes the proof. 
 \end{proof}
 
 
 \begin{corollary}
   If $A$ is irreducible then $I+A$ is primitive. 
 \end{corollary}
 \begin{corollary}
   $A^T$ is irreducible whenever $A$ is irreducible.
 \end{corollary}
 \begin{property}\label{geenzerorij}
  No row or column of an irreducible matrix $A$ can vanish. This means that $A$ cannot have a row or a column of zeros.
   \end{property}
 \begin{proof}
   Suppose that $A$ has a zero row, then it could be permuted to
     $$PAP^T = \begin{pmatrix}  0 & 0 \ldots 0\\
\begin{tabular}{l}
c_1  \\
\vdots \\
c_n\\   
\end{tabular}
 & D\\
 \end{pmatrix} $$
 by some permutation matrix $P$. It follows from Definition \ref{defreduciebel} 
 that $A$ is reducible. Similarly, if $A$ has zero column, it can be permuted 
 to
   $$PAP^T = \begin{pmatrix} B & \begin{tabular}{l}
0 \\
\vdots \\
0\\   
\end{tabular}\\
    c_1  \ldots c_n   & 0\\
    \end{pmatrix} $$
    again from Definition \ref{defreduciebel} we conclude that $A$ is reducible.

 \end{proof}
 

 \section{Perron-Frobenius Theorem}
 
 \subsection{Spectral radii of nonnegative matrices}
 
 \begin{definition}
  Let A be an $n\times n$-matrix with complex entries and eigenvalues $\lambda_i$,
  $1 \leq i \leq n$. Then:
  $$\rho(A) = \max_{1 \leq i \leq n} |\lambda_i|$$
  is called the \textbf{spectral radius} of the matrix $A$.
\end{definition}

Geometrically, if all the eigenvalues $\lambda_i$ of $A$ are plotted in the 
complex plane, then $\rho(A)$ is the radius of the smallest disk $|z| \leq 
R$, with center at the origin, which includes all the eigenvalues of the matrix 
$A$.

 We now establish a series of lemmas on nonnegative irreducible square matrices. 
 These lemmas will allow us to prove the Perron-Frobenius at the end of this
 section.\\
 
 If $A \geq 0$ is an irreducible $n\times n$-matrix and $\mathbf{x}$, a vector of size $n$ with \mathbf{0} \not = \mathbf{x} \geq 0$, let
 \begin{eqnarray}
r_\mathbf{x} = \min\left\{\frac{\sum^n_{j=1}a_{ij}x_j}{x_i}\right\}
  \end{eqnarray}
 where the minimum is taken over all $i$ for which $x_i > 0$. Clearly, $r_\mathbf{x}$ is 
 a nonnegative real number and is the supremum of all $p \geq 0$ for 
 which 
  \begin{eqnarray}\label{eenvoudigefrob}
A\mathbf{x} \geq p\mathbf{x}  \end{eqnarray}
 
  We now consider the nonnegative quantity $r$ defined by 
 \begin{eqnarray}\label{eenvoudigefrob2}\label{quantityr}
   r = \sup_{\begin{subarray}{l}\mathbf{x} \geq 0\\
    \mathbf{x}\not = \mathbf{0}\end{subarray}} \{r_\mathbf{x}\}
 \end{eqnarray}
As $r_{\mathbf{x}}$ and $r_{\alpha\mathbf{x}}$ 
have the same value for any 
scalar $\alpha > 0$, we need consider only the set $B$ of vectors $\mathbf{x} \geq 0$ 
with $||\mathbf{x}|| = 1$, and we correspondingly let $Q$ be the set of all vectors $\mathbf{y}=(I+A)^{n-1}\mathbf{x}$ 
where $\mathbf{x} \in B$. From Theorem \ref{irredubieleegenschappen} , $Q$ 
consists only of positive vectors. Multiplying both sides of the inequality $A\mathbf{x} \geq r_{\mathbf{x}}\mathbf{x}$
by $(I+A)^{n-1}$, we obtain:

$$\forall \mathbf{y} \in Q: A\mathbf{y} \geq r_{\mathbf{x}}\mathbf{y},$$
and we conclude from (\ref{eenvoudigefrob}) that $r_\mathbf{y} \geq 
r_{\mathbf{x}}$. Therefore, the quantity $r$ of (\ref{eenvoudigefrob2}) can be 
defined equivalently as:
\begin{eqnarray}\label{quantityr2}
  r = \sup_{\mathbf{y} \in Q} \{r_{\mathbf{y}}\}
\end{eqnarray}
As $B$ is a compact set (in the usual topology) of vectors, so is $Q$, and as $r_\mathbf{y}$ is a continuous 
function on $Q$, we know from the extreme value theorem that there necessarily exists a positive vector $\mathbf{z}$ for which:
\begin{eqnarray}\label{frobbrol3}
  A\mathbf{z} \geq r\mathbf{z},
 \end{eqnarray}
 and no vector $\mathbf{w} \geq 0$ exists for which $A\mathbf{w} > r\mathbf{w}$. 
\begin{definition}
  We call all nonnegative, nonzero vectors $\mathbf{z}$ satisfying 
  (\ref{frobbrol3})
  \textbf{extremal vectors} of the matrix $A$.
\end{definition}

\begin{lemma}
  If $A \geq 0$ is an irreducible $n\times n$-matrix, the quantity $r$ of (\ref{quantityr}) 
  is positive.
\end{lemma}
\begin{proof}
  If $\mathbf{x}$ is the positive vector whose coordinates are all unity, then since the matrix $A$ is irreducible, 
  we know from Property \ref{geenzerorij} that no row of $A$ can vanish, and consequently no component of $A\mathbf{x}$ 
  can vanish. Thus, $r_\mathbf{x} > 0$, proving that $r > 0$.
  \end{proof}
  
  \begin{lemma}\label{lemmafrobextremal} If $A \geq 0$ is an irreducible $n\times n$-matrix, each extremal vector $\mathbf{z}$ is a positive 
  eigenvector of $A$ with corresponding eigenvalue $r$ of (\ref{quantityr}), i.e., $A\mathbf{z} 
  = r\mathbf{z}$ and $\mathbf{z} > 0$.
  \end{lemma}
  \begin{proof}
    Let $\mathbf{z}$ be an extremal vector with $A\mathbf{z} - r \mathbf{z}= 
    \mathbf{t}$. If $\mathbf{t}\not = \mathbf{0}$, then some coordinate of $\mathbf{t}$ is 
    positive; multiplying through by the matrix $(I+A)^{n-1}$, we have:
    $$A\mathbf{w} - r\mathbf{w} > 0, \text{ with } \mathbf{w} = (I+A)^{n-1}\mathbf{z}$$
    from Theorem \ref{irredubieleegenschappen} we know that $\mathbf{w} > 0$. It 
    would then follow that $r_{\mathbf{w}} > r$, contradicting the definition of 
    $r$ in (\ref{quantityr2}). Thus $A\mathbf{z} = r\mathbf{z}$, and since 
    $\mathbf{w} > 0$ and $\mathbf{w} = (1+r)^{n-1}\mathbf{z}$, then we have $\mathbf{z} 
    > 0$, completing the proof.
  \end{proof}
  
  \begin{lemma}\label{lemma2frobvolledig}
    Let $A \geq 0$ be an irreducible $n\times n$-matrix, and let $B$ be an $n\times 
    n$- complex matrix with $|B|\leq A$. If $\beta$ is any eigenvalue of $B$, 
    then 
\begin{eqnarray}\label{lemma2frob}
|\beta| \leq r,  
\end{eqnarray}
where $r$ is the positive quantity of (\ref{quantityr}). Moreover, equality is 
valid in (\ref{lemma2frob}), i.e., $\beta = re^{i\phi}$, if and only if $|B| = 
A$, and where $B$ has the form:
\begin{eqnarray}\label{lemma2frobvormB}
B = e^{i\phi}DAD^{-1},
\end{eqnarray}
and $D$ is a diagonal matrix whose diagonal entries have modulus unity.
  \end{lemma}
  
  
  \begin{proof}
  If $\beta\mathbf{y} = B\mathbf{y}$ where $\mathbf{y} \not = \mathbf{0}$, then
  $$\beta y_i = \sum_{j=1}^n b_{ij}y_i, \text{ with } 1 \leq i \leq n.$$
  Using the hypotheses of the lemma and the notation of Definition \ref{modulusmatrix}, it follows that:
  \begin{eqnarray}
|\beta||\mathbf{y}| \leq |B||\mathbf{y}| \leq A |\mathbf{y}|,
\end{eqnarray}
which implies that $|\beta| \leq r_{|\mathbf{y}|} \leq r$, proving 
(\ref{lemma2frob}). If $|\beta| = r$, then $|\mathbf{y}|$ is an extremal vector 
of $A$. Therefore, from Lemma \ref{lemmafrobextremal}, $|\mathbf{y}|$ is a 
positive eigenvector of $A$ corresponding to the positive eigenvalue $r$. Thus,
\begin{eqnarray}\label{lemma2frobconclusion}
  r|\mathbf{y}| = |B||\mathbf{y}| = A|\mathbf{y}|,
\end{eqnarray}
and since $|\mathbf{y}| > 0$, we conclude from (\ref{lemma2frobconclusion}) and 
the hypothesis $|B| \leq A$ that 
\begin{eqnarray}\label{lemma2frobBeqA}  |B| = A
  \end{eqnarray}
For the vector $\mathbf{y}$, where $|\mathbf{y}| > 0$, let
$$D = \text{diag}\left\{\frac{y_1}{|y_1|}, \ldots, \frac{y_n}{|y_n|}\right\}.$$
It is clear that the diagonal entries of $D$ have modulus unity, and
\begin{eqnarray}
  \mathbf{y} = D|\mathbf{y}|.
    \end{eqnarray}
Setting $\beta = re^{i\phi}$, then $B\mathbf{y} = \beta\mathbf{y}$ can be 
written as:
\begin{eqnarray}\label{lemma2frobC}
  C|\mathbf{y}| = r|\mathbf{y}|,
\end{eqnarray}
where
\begin{eqnarray}\label{lemma2frobdefC}
  C = e^{-i\phi}D^{-1}BD.
\end{eqnarray}
From (\ref{lemma2frobconclusion}) and (\ref{lemma2frobC}), equiting terms equal 
to $r|\mathbf{y}|$ we have
\begin{eqnarray}\label{lemma2frobequalsmatriceswithy}
 C|\mathbf{y}| = |B||\mathbf{y}| = A|\mathbf{y}|.
 \end{eqnarray}
 From the definition of the matrix $C$ in (\ref{lemma2frobdefC}), $|C| = |B|$. 
 Combining with (\ref{lemma2frobBeqA}), we have:
 \begin{eqnarray}\label{lemma2frobequalsmatrices}
 |C| = |B| = A.
 \end{eqnarray}
 Thus, from (\ref{lemma2frobequalsmatriceswithy}) we conclude that $C|\mathbf{y}| = 
 |C||\mathbf{y}|$, and as $|\mathbf{y}| > 0$, it follows that $C = |C|$ and thus $C = A$ from  
 (\ref{lemma2frobequalsmatrices}). Combining this result with 
 (\ref{lemma2frobdefC}), gives the desired result that $B = e^{i\phi}DAD^{-1}$. 
 Conversely, it is obvious that if $B$ has the form in (\ref{lemma2frobvormB}), 
 then $|B|=A$, and $B$ has an eigenvalue $\beta$ with $|\beta| = r$, which 
 completes the proof.
\end{proof}

\begin{corollary}\label{lemma2frobcorollary}
  If $A \geq 0$ is an irreducible $n\times n$-matrix, then the positive 
  eigenvalue $r$ of Lemma \ref{lemmafrobextremal} equals the spectral radius $\rho(A)$ 
  of $A$
\end{corollary}  
\begin{proof}
  Setting $B = A$ in Lemma \ref{lemma2frobvolledig} immediately gives us this
  result.
\end{proof}
 In other words, if $A \geq 0$ is an irreducible $n\times n$-matrix, its 
 spectral radius $\rho(A)$ is positive, and the intersection in the complex 
 plane of the circle $|z| = \rho(A)$ with the positive real axis is an 
 eigenvalue of $A$.
 \begin{definition}
   A \textbf{principal square submatrix} of an $n\times n$-matrix A is any 
   matrix obtained by crossing out any $j$ rows and the corresponding $j$
 columns of $A$, with $1 \leq j \leq n.$ \end{definition}
 \begin{lemma}\label{froblemma4}
   If $A \geq 0$ is an irreducible $n\times n$-matrix, and $B$ is any principal 
   square submatrix of $A$, then $\rho(B) < \rho(A).$
 \end{lemma}
 \begin{proof}
   If $B$ is any principal submatrix of $A$, then there is an $n\times 
   n$-permutation matrix $P$ such that $B = A_{11}$ where
   \begin{eqnarray}
    C = \begin{pmatrix}  A_{11}  & 0\\
 0  & 0\\
\end{pmatrix};
PAP^T =  \begin{pmatrix}  A_{11}  & A_{12}\\
 A_{21}  & A_{22}\\
\end{pmatrix}
  \end{eqnarray}
Here, $A_{11}$ and $A_{22}$ are, respectively, $m\times m$ and $(n-m) \times (n-m)$ 
principal square submatrices of $PAP^T$, $1 \leq m \leq n.$ Clearly, $0\leq C \leq 
PAP^T$, and $\rho(C) = \rho(B) = \rho(A_{11})$, but as $C = |C| \not = PAP^T$, 
the conclusion follows immediately from Lemma \ref{lemma2frobvolledig} and 
Corollary \ref{lemma2frobcorollary}.
 
  \end{proof}


   The following lemma is used to prove that $\rho(A)$ is a simple 
   eigenvalue of $A$ in the Perron-Frobenius theorem. The proof uses the 
   extension of the product rule of derivation for multilinear functions 
   $M(a_1,\ldots,a_k)$. Suppose $x_1,..,x_k$ are differentiable vector functions, 
   then $M(x_1,\ldots,x_k)$ is differentiable and:
   $$\frac{\mathrm{d}}{\mathrm{d}t}M(x_1,\ldotsx_k) = M(\frac{\mathrm{d}}{\mathrm{d}t} x_1, x_2, \ldots, x_k) 
   + M(x_1, \frac{\mathrm{d}}{\mathrm{d}t} x_2, \ldots, x_k) + \ldots +  M(x_1,  x_2, \ldots,\frac{\mathrm{d}}{\mathrm{d}t} x_k) $$
   The most important application of this rule is for the derivative of the determinant:
   $$\frac{\mathrm{d}}{\mathrm{d}t}\det(x_1,\ldots, x_k) = \det(\frac{\mathrm{d}}{\mathrm{d}t} x_1, x_2, \ldots, x_k) 
   + \det(x_1, \frac{\mathrm{d}}{\mathrm{d}t} x_2, \ldots, x_k) + \ldots + \det(x_1,  x_2, \ldots,\frac{\mathrm{d}}{\mathrm{d}t} x_k) $$

 \begin{lemma}\label{maclane}

   Let $A$ be an $n \times n$-matrix over the complex numbers and let $\phi(A, \lambda) = \det(\lambda I_n - A)$
   be the characteristic polynomial of $A$. Let $B_i$ be the principal submatrix 
   of $A$ formed by deleting the $i$-th row and column of $A$ and let $\phi(B_i, \lambda)$ 
   be the characteristic polynomial of $B_i$. Then:
   $$\phi'(A, \lambda) = \frac{\mathrm{d}\phi(A, \lambda)}{\mathrm{d}\lambda}= 
   \sum_i \phi(B_i, \lambda)$$
 \end{lemma}
\begin{proof}
  The proof is immediately done by direct computation:
  
  $$\phi(A, \lambda) = \det\begin{bmatrix} 
\lambda -a_{1,1} & -a_{1,2}  & \ldots & -a_{1,n}   \\
-a_{2,1}  & \lambda - a_{2,2} & \ldots & -a_{2,n}   \\
\vdots & \vdots & \ddots & \vdots  \\
- a_{n,1}  & -a_{n,2} & \ldots & \lambda - a_{n,n}  
\end{bmatrix}.$$
Using the extension of the product rule of derivation for multilinear functions
\begin{eqnarray*}
  \phi'(A, \lambda) = \det\begin{bmatrix} 
1 & 0  & \ldots & 0    \\
-a_{2,1}  & \lambda - a_{2,2} & \ldots & -a_{2,n}   \\
\vdots & \vdots & \ddots & \vdots  \\
- a_{n,1}  & -a_{n,2} & \ldots & \lambda - a_{n,n}  
\end{bmatrix} + 
\det\begin{bmatrix} 
\lambda -a_{1,1} & -a_{1,2}  & \ldots & -a_{1,n}   \\
0  & 1 & \ldots &  0   \\
\vdots & \vdots & \ddots & \vdots  \\
- a_{n,1}  & -a_{n,2} & \ldots & \lambda - a_{n,n}  
\end{bmatrix} + \ldots \\
+ \det\begin{bmatrix} 
\lambda -a_{1,1} & -a_{1,2}  & \ldots & -a_{1,n}   \\
-a_{2,1}  & \lambda - a_{2,2} & \ldots & -a_{2,n}   \\
\vdots & \vdots & \ddots & \vdots  \\
0  & 0 & \ldots & 1
\end{bmatrix} = \sum_i \phi(B_i, \lambda).
\end{eqnarray*}
\end{proof}
We now collect the above results into the following main theorem: we finally 
arrived at the Perron-Frobenius Theorem:
\begin{theorem}(\textbf{Perron-Frobenius theorem})\label{frobtheorem}
  Let $A \geq 0$ be an irreducible $n\times n$-matrix. Then,
  \begin{enumerate}
    \item $A$ has a positive real eigenvalue equal to its spectral radius.
    \item To $\rho(A)$ there corresponds an eigenvector $\mathbf{x} > 0$.
    \item $\rho(A)$ increases when any entry of $A$ increases.
    \item $\rho(A)$ is a simple eigenvalue of $A$.
    \item If $A\mathbf{x} = \rho(A)\mathbf{x}$ where $\mathbf{x} > 0$ and $\mathbf{x}$ 
    is a normalized vector, then $\mathbf{x}$ is unique.
    
  \end{enumerate}
  \end{theorem}
 \begin{proof}
  (1) and (2) follow immediately from Lemma \ref{lemmafrobextremal} and 
   Corollary \ref{lemma2frobcorollary}. 
   
  (3) Suppose we increase 
   some entry of the matrix $A$, giving us a new irreducible matrix $\tilde{A}$ 
   where $\tilde{A} \geq A$ and  $\tilde{A} \not = A$. Applying Lemma 
   \ref{lemma2frobvolledig}, we conclude that $\rho(\tilde{A}) > \rho(A).$ 
   
   (4) $\rho(A)$ is a simple eigenvalue of $A$, i.e., $\rho(A)$ is a zero of multiplicity one of the characteristic polynomial
   $\phi(\lambda) = \det(\lambda I_n - A)$, we make use of Lemma \ref{maclane} by using the fact that $\phi'(\lambda)$ is the sum of the
   determinants of the principal $(n-1)\times(n-1)$ submatrices of $\lambda I - A$. If $A_i$ is any principal submatrix of
   $A$, then from Lemma \ref{froblemma4}, $\det(\lambda I - A_i)$ (with $I$ the identity matrix with the same size as the principal submatrix $A_i$) cannot vanish 
   for any $\lambda \geq \rho(A)$. From this it follows that:
   $$\det(\rho(A)I - A_i) > 0,$$
   and thus
   $$\phi'(\rho(A)) > 0.$$
   Consequently, $\rho(A)$ cannot be z zero of $\phi(\lambda)$ of multiplicity 
   greater than one and thus $\rho(A)$ is a simple eigenvalue of $A$. 
   
  (5) If $A\mathbf{x} = \rho(A)\mathbf{x}$ where $\mathbf{x} > 0$ 
   and $||x|| = 1$ ($||x||$ denotes the standard Euclidean norm), we cannot find 
   another eigenvector $\mathbf{y} \not = s\mathbf{x}$, with $s$ a scalar, of $A$ 
   with $A\mathbf{y} = \rho(A)\mathbf{y}$, so that the eigenvector $\mathbf{x}$ 
   , meaning that the normalized eigenvector $\mathbf{x}$ is uniquely 
   determined.
       \end{proof}

   
   





\subsection{Example}
To check wether a matrix with nonnegative entries is primitive, irreducible or 
neither, we just have to replace all nonzero entries by 1 since this does not 
affect the classification. The matrix
$$\left(\begin{array}{cc}
1 & 1\\
1 & 1 \\
\end{array}\right)$$
is strictly positive and thus primitive. The matrices
$$\left(\begin{array}{cc}
1 & 0\\
1 & 1 \\
\end{array}\right) \text{\;\;\; and \;\;\,} 
\left(\begin{array}{cc}
1 & 1\\
0 & 1 \\
\end{array}\right)$$
both have 1 as a double eigenvalue hence can not be irreducible.
The matrix $\left(\begin{array}{cc}
1 & 1\\
1 & 0 \\
\end{array}\right)$ satisfies:
$$\left(\begin{array}{cc}
1 & 1\\
1 & 0 \\
\end{array}\right)^2 = \left(\begin{array}{cc}
2 & 1\\
1 & 1 \\
\end{array}\right)$$
and hence is primitive. The same goes for $$\left(\begin{array}{cc}
0 & 1\\
1 & 1 \\
\end{array}\right),$$ this matrix is irreducible but not primitive. Its 
eigenvalues are $1$ and $-1$.
\section{Graphs}
After introducing different kinds of matrices and proving the Perron-Frobenius 
theorem, we now take a closer look at graphs. Here too, we'll look at different 
families of graphs and prove some relevant properties about them. We also link 
the concept of graphs with different kinds of matrices, deepening our 
insight of some theorems of the previous section.\\

The definitions and results in this section are mainly based on the course 
`Discrete Mathematics' by P. Cara \cite{cara}. 


\subsection{General definitions}
\begin{definition}\label{defgraf}
  A \textbf{graph} is a an ordered pair $(V,\rightarrow)$ where $V$ is a set and $\to$ is a relation. The elements of $V$ are called
 \textbf{vertices} and $\rightarrow$ is called the \textbf{adjacency relation}. Let $u, v \in V$, then the couple 
 $(u, v)$ belonging to $\to$ is called an \textbf{arc} or \textbf{edge} and we write $u \rightarrow v$. We also
 say that $u$ is \textbf{adjacent} to $v$.  When $v \to v$ (with $v \in V$) we say that the graph has a 
 \textbf{loop} in $v$. A graph is 
  most of the time denoted by calligraphic letters $\mathscr{G}, 
  \mathscr{H},..$. Therefore we denote $\mathscr{G} (V,\rightarrow)$ for the graph $\graf$ with vertex set $V$ and adjancency relation $\to$. 
  When handling multiple graphs, $V(\mathscr{G})$ denotes the set of vertices of 
  $\mathscr{G}$.
  \end{definition}
 When the relation $\rightarrow$ is symmetric, 
 we call the graph \textbf{undirected}, in this case we often write $\sim$ instead 
 of $\rightarrow$.  
 
 
 \begin{example}\label{simpelegraf}
  The graph
  \begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]
  \node[main node] (1) [label=above:$v_1$] {};
  \node[main node] (3) [label=right:$v_3$][below right of=1] {};
    \node[main node] (2) [label=left:$v_2$][below left of=1] {};

  \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
       (2) edge node [left] {} (1)
      (2) edge node [left] {} (3)
      (3) edge node [left] {} (1)
      (3) edge node [left] {} (2)
\end{tikzpicture}

\end{center}
is an undirected graph with vertices $v_1, v_2, v_3$. The adjacency 
relation $\to$ equals \\$\{(v_1,v_2), (v_2,v_1), (v_2, v_3), (v_3,v_2), (v_3, v_1), 
(v_1,v_3)\}$.
\end{example}

There is a small problem with our definition, because not all graphs are taken 
into account, by example the  graph below is not a graph following our definition 
because you can not define multiple edges between vertices in a relation. 

  \begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]


  \node[main node] (1) [label=above:$v_1$] {};
  \node[main node] (3) [label=right:$v_3$][below right of=1] {};
    \node[main node] (2) [label=left:$v_2$][below left of=1] {};

  \path[every node/.style={font=\sffamily\small}]
   
  (1) edge node {} (2)
    (2) edge node {} (1)
    (2) edge node {} (3)
      (3) edge node {} (2)
    (3) edge [bend right] node {} (1)
    (3) edge [bend left] node {} (1);
    
\end{tikzpicture}
\end{center}




Therefore we introduce a more general definition and introduce the concept of 
multiplicity of an edge:

\begin{definition}
  A graph is an ordered pair $(V, \mu)$ with $V$ a set and $\mu: V \times V \to \N$ a function that gives the \textbf{multiplicity} of an edge.
  The function is defined as follows:
  \begin{itemize}
    \item when $\mu(u, v) = 0$ we say that $u$ and $v$ are not adjacent;
    \item when $\mu(u, v) = k > 0$ we say that there are $k$ edges from $u$ to 
    $v$.
  \end{itemize} 
\end{definition}
It is clear that our previous definition fits perfectly in this more general definition, by constructing $\mu$ in this case as follows:
$$\mu(u,v)= \begin{cases} 1 &\mbox{if } (u,v) \in \to \\ 
0 & \mbox{otherwise}. \end{cases}$$ 
 \begin{definition}
  The \textbf{neighbourhood} of a vertex $v$ of a graph $\graf(V,\mu)$ is the induced subgraph $\graf_v$ 
 with vertex set $V'$ consisting of all vertices adjacent to $v$ without $v$ itself and with the multiplicity function $\mu'$,
 which is the restriction of $\mu$ to the vertices in $V'$.
 A vertex with a neighbourhood equal to the empty graph (a graph with an empty set of vertices) is called 
 \emph{isolated}.
\end{definition}  
  

 
 
\begin{definition}
  The \textbf{order of a finite graph $\graf$} is the number of vertices of $\graf$ and is denoted by $|\graf|$.  
\end{definition} 
  
  \begin{definition}
    The \textbf{degree of a vertex $v$ in a graph $\graf$} is the number of edges 
    containing $v$ and is denoted by $\deg(v)$, so:
    $$\deg{v} = |\graf_v|$$
  \end{definition}
  
 
 \begin{definition}
   A  \textbf{walk} in a graph $\graf$ is a sequence of vertices
   $$v_0,v_1,\ldots,v_k$$
   such that $v_{i-1} \to v_i$ for each $i \in \{1,\ldots,k\}$. The \textbf{length} of the walk is $k$, one less
   than the number of vertices. 
 \end{definition}
 
 \begin{definition}
   If all edges are distinct in a  walk in a graph $\graf$, we call the walk a \textbf{path}.
 \end{definition}
 
 \begin{definition}
   A \textbf{cycle} is a walk from $v_0$ to $v_0$ in which all vertices except $v_0$ are distinct. \end{definition}
 
 \begin{definition}
   A graph $\graf$ is \textbf{connected} if it possible to establish a path from any 
   vertex to any other vertex.
    \end{definition}
 \subsubsection{Product graphs}
 \begin{definition}
   Take two graphs $\graf(U,\to), \grafeen(V,\to')$, the \textbf{product graph} $\graf \times \grafeen$ 
   is the graph with $|\graf|.|\grafeen|$ vertices and that has an edge between 
   vertices $(u_i, v_j)$ and $(u_k, v_l)$ if there is an edge between $u_i$ and $u_k$ in $\graf$ and there is an 
   edge between $v_j$ and $v_l$ in $\grafeen$. 
 \end{definition}
 \subsubsection{Adjacency matrices}
 We now represent an undirected graph in the form of an adjacency matrix. This 
 matrix gives a lot of useful information about the graph and vice versa.
 
 
 \begin{definition}
 Let $\graf(V, \sim)$ be an undirected graph of order $n$ and define a numbering on the 
 vertices $v_1,\ldots, v_n$. Then the \textbf{adjacency matrix} $A_\graf$ of $\graf$ is the real
 $n\times n$-matrix with $a_{ij}$ equal to the number of edges between $i$ and $j$.
 \end{definition}
 
\begin{theorem}
  Let $k > 0$. The element on place $(i,j)$ in $A^k_\graf$ contains the number 
  of walks of length $k$ from $i$ to $j$.
\end{theorem}
 \begin{proof}
   By induction on $k$. 
   
   For $k = 1$ we count the walks of length $1$. These are edges and the 
   result follows immediately from the defintion of $A_\graf$.
   
  Let $v_l$ be a vertex of $\graf$. If there are $b_{ij}$ walks of length $k$ 
  from $i$ to $l$ and $a_{lj}$ walks of length 1 (edges) from $v_l$ to $v_j$, 
  then there are $b_{il}a_{lj}$ walks of length $k+1$ from $v_i$ to $v_j$ passing vertex 
  $v_l$. Therefore, the number of walks of length $k+1$ between $v_i$ and $v_j$ 
  is equal to:
  $$\sum_{l\in V(\graf)} b_{il}a_{lj} =: c_{ij}.$$
By the induction hypothesis we now that $b_{il}$ equals  the element on place $(i,l)$ 
in $A^k_\graf$ so $c_{ij}$ is exactly the element on place $(i,j)$ in the matrix 
product
$$A^k_\graf A_\graf = A^{k+1}_\graf.$$
 \end{proof}
 \begin{example}
The adjacency matrix of the graph in Example \ref{simpelegraf} is:
$$A = \begin{pmatrix}
0 & 1 & 1\\
1 & 0 & 1\\
1 & 1 & 0
\end{pmatrix}


\end{example}
\subsection{Simple graphs}
We now introduce simple graphs and show that the adjacency matrix of a simple graph $\graf$ is irreducible if and only 
if $\graf$ is connected. 
\begin{definition}
  A \textbf{simple graph} is an undirected graph $\graf(V,\mu)$ containing no loops and for all vertices $v_i, v_j \in V$, we have that the multiplicity
 $\mu(v_i,v_j)$ is at most 1.\end{definition}
 
 \begin{theorem}
   Let $\graf$ be a simple graph with $n$ vertices and adjacency matrix $A$. 
   Then $\graf$ is connected if and only if $A$ is irreducible.
 \end{theorem}
 
 \begin{proof}
   A path between two vertices in the simple graph $\graf$ has at most $n-1$ 
   edges. So, $\graf$ is connected if and only if $\forall i, j \in V(\graf)$ there exists 
   a $k \leq n- 1$ with a path of length $k$ from $i$ to $j$. So $\forall i, j \in V(\graf): \exists k < n: (A^k_\graf)_{ij} > 
   0$. Because $(I_n + A)^{n-1} = \sum^{n-1}_{k=0}{n-1 \choose k}A^k$, we know from Theorem \ref{irredubieleegenschappen} that
$A$ is irreducible.   
 \end{proof}
\subsection{Directed graphs}
In this section, we take a closer look at \emph{directed graphs}. We already 
showed that the adjacency matrix of a simple graph $\graf$ is irreducible if and only 
if $\graf$ is connected. But this is not appropriate for checking the irreducibility 
of matrices using graphs, because the only matrices we can check
are the ones containing entries equal to $0$ or $1$. Can we find a more general method for checking irreducibility of 
a matrix using graphs? Luckily, we can and that's where directed graphs come 
in: we will introduce a method for turning a matrix in to a directed graph 
and vice versa and link the irreducibility of this matrix to a property of the 
directed graph.
\begin{definition}
  A graph $\graf(V,\to)$ is \textbf{directed} when the relation $\to$ is not symmetric.
    A directed graph $\graf(V, \to)$ is \textbf{connected} if the underlying undirected graph
  (remove all arrows on the edges) is connected. $\graf$ is \textbf{strongly connected} if for any two
  vertices $v_i$ and $v_j$ there exists a \textbf{directed path}:
  $$v_i \rightarrow v_{l_1}, v_{l_1} \rightarrow v_{l_2}, \ldots, v_{l_{r-1}} 
  \rightarrow v_{l_r = j}.$$

 \end{definition}

We now link directed graphs with $n \times n$-matrices and vice versa. Consider any $n \times n$-matrix $A$, and consider any $n$ distinct 
points $v_1, v_2,\ldots,v_n$ in the plane, which will be the nodes of the directed graph. 
For every nonzero entry $a_{ij}$ of the matrix, we connect the node $v_i$ to the 
node $v_j$ by means of an arc $v_i \rightarrow v_j$ directed from $v_i$ to 
$v_j$. In this way, with every $n \times n$-matrix $A$ can be associated a \textbf{directed} 
graph $\graf(A)$.

\begin{example}\label{vbgraf}
  Consider the matrix:
  $$B = \begin{pmatrix} 
0 & 0 & 89 & 7  \\
0 & 0 & 2 & 2  \\
123 & 9 & 0 & 0  \\
14 & 89 & 0 & 0  
\end{pmatrix}.$$
We get the directed graph: \\
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]


  \node[main node] (1) [label=left:$v_1$] {};
  \node[main node] (3) [label=right:$v_3$][right of=1] {};
    \node[main node] (2) [label=right:$v_2$][below of=3] {};
  \node[main node] (4) [label=left:$v_4$][below of=1] {};

  \path[every node/.style={font=\sffamily\small}]
   
    (1) edge [bend right] node [left] {} (3)
  
    (3) edge [bend right] node [right] {} (1)
    (1) edge [bend right] node [left] {} (4)
  
    (4) edge [bend right] node [right] {} (1)
        (3) edge [bend right] node [left] {} (2)
  
    (2) edge [bend right] node [right] {} (3)
       (4) edge [bend right] node [left] {} (2)
  
    (2) edge [bend right] node [right] {} (4)


\end{tikzpicture}
\end{center}
Notice that this graph is strongly connected.

\end{example}
In the next proof, we study the equivalence of the matrix property of 
irreducibility of  Definition \ref{defreduciebel} with the concept of the 
strongly connected directed graphs of a matrix:
\begin{theorem}
  An $n \times n$-matrix $A$ is irreducible if and only if its directed 
  graph $\graf(A)$ is strongly connected.
\end{theorem}
\begin{proof}
  First assume that $A$ is reducible. Consider:
   $$ PAP^T = \begin{pmatrix}  B  & 0\\
 C  & D\\
\end{pmatrix} $$
where $D$ is a $k \times k$-submatrix and $1 \leq k \leq n-1$, and $B$ is a square matrix of size at least $1$. 
Consider any directed path from vertex $v_i$ with $k < i$. The last segment of the path is determined by the presence of a 
nonzero element $PAP^T_{ij}$ in the $i$th row of $ PAP^T$. This row has zeros in the last 
$k$ entries, so it is possible to make a connection from node $i$ to node $j$ 
only if $j < k$. Similarly, the path can be extended from node $j$ only to 
another node smaller than $k$. Continuing in this way, we found that a 
directed path from node $i$ with $i < k$ cannot be connected to a node greater than $k +1$. Hence the
directed graph $\graf(PAP^T)$ is not strongly connected. The directed graph $\graf(A)$ 
is also not connected: observe that the graph of  $PAP^T$ is obtained from that of $A$ just by renumbering the nodes, and
this operation does not affect the connectedness of the graph. 

Now assume that $\graf(A)$ is not strongly connected. Then there are distinct vertices $v_i$ and 
$v_j$ of $\graf(A)$ for which there is no directed path from $v_i$ to $v_j$. Let $W_1$ consist of
$v_j$ and all vertices from which there is directed path to $v_j$, let $W_2$ consist of $v_i$ and
all vertices to which there is a directed path from $v_i$. The sets $W_1$ and $W_2$ 
are nonempty and disjoint. Let $W_3$ be the set consisting of those vertices 
which belong to neither $W_1$ nor $W_2$ ($W_3 = V\backslash(W_1 \bigcup W_2)$). 
We now permute $A$ so that the rows and columns corresponding to the 
vertices in $W_2$ come first followed by those corresponding to the vertices in 
$W_3$:
$$\bordermatrix{\; &W_2&W_3&W_1\cr
                W_2&B_1 &  B_2  & O_{1}\cr
                W_3&B_3 &  B_4  & O_{2}\cr
                W_1&C_3 &  C_4 & D}$$
Since there is no directed walk from $v_i$ to $v_j$ there is no arc
from a vertex in $W_2$ to a vertex in $W_1$. Also there is no arc from
a vertex $v_k$ in $W_3$ to a vertex in $W_1$, because such an arc implies that $v_k$ 
belongs to $W_1$. Hence $O_{1} = 0$ and $O_{2} = 0$, D is a square matrix
and so is
$$B = \bordermatrix{\; &W_2&W_3\cr
                W_2&B_1 &  B_2\cr
                W_3&B_3 &  B_4}.$$
                Hence,   $A$ is reducible.
\end{proof}
\chapter{Similarity on graphs}

In the previous chapter all the basic terminology and results were introduced, 
now we take an extensive look at the concept of similarity on graphs. In the 
first section, we introduce similarity on directed graphs. More precisely, we 
define a similarity matrix $S$ of to matrixes $\graf, \grafeen$ whose real entries $s_{ij}$ express how similar a 
vertex $v_j$ of $\graf$ is to vertex $v'_i$ of $\grafeen$. In the second 
section, we take a closer look at similarity on colored graphs. Both sections 
conclude with an extensive application of the results.

\section{Similarity on directed graphs}
This section summarizes the paper `\emph{A Measure of Similarity between Graph Vertices: 
Applications to Synonym Extraction and Web Searching}'\cite{blondel} of V. D. Blondel e.a. 
An overview of the idea and results of the paper is presented, but the 
proofs are developed far more extensively than they are are in te paper. The 
paper concludes with a detailed example about the automatic extraction of 
synonyms in a monolingual dictionary, but this example is left out and replaced 
by an own example about .... .

\subsection{Introduction}
\subsubsection{The method of Kleinberg}
The concept of similarity between directed graphs arises as a generalization of 
hubs and authorities introduced by Kleinberg\cite{kleinberg}. Web searching engines like Google are using graphs with 
vertices and edges that represent the links between pages on the web. To get 
information out af such a graph, the idea is to identify in a set of pages 
relevant to a query search, the subset of pages that are good \emph{hubs} and the 
subset of pages that are good \emph{authorities}. For example, searching for 
`super markets', the web-pages of Carrefour, Colruyt,... and other super markets 
are good authorities, whereas web-pages that point to these home-pages (like Test Aankoop, websites with 
recipes,...) are good hubs. A good hub points to good authorities and vice versa. 
The whole idea of Kleinberg was to derive an iterative method that assigns an
`authority score' and a `hub score' to every vertex of a given graph. These 
scores can be obtained as the limit of a converging iterative process. We will 
now describe this iterative process.\\

Let $\graf(V,\to)$ be a graph and let $h_j$ and $a_j$ be the hub and authority 
scores of vertex $v_j$. These scores must be initialized by some positive start values 
and then updated simultaneously for all vertices. This leads to a \emph{mutually reinforcing relation} 
in which the hub score of $v_j$ is set equal to the authority scores of all 
vertices pointed to by $v_j$ and in an equal manner the authority score of $v_j$ 
is set equal to the sum of the hub scores of all vertices pointing to $v_j$.

$$\begin{cases} h_j := \sum_{i:(v_j,v_i)\in \to} a_i,\\ 
a_j := \sum_{i:(v_i,v_j)\in \to} h_i.
\end{cases}$$ 

Let $B$ be the adjacency matrix of $\graf$ and let $h$ and $a$ be the vectors of 
hub and authority scores. The mutually reinforcing relation can now be rewritten 
in:

$$\begin{pmatrix} 
h\\
a
\end{pmatrix}_{k+1} = \begin{pmatrix} 
0 & B\\
B^T & 0
\end{pmatrix} \begin{pmatrix} 
h\\
a
\end{pmatrix}_{k}, k = 0, 1,\ldots,$$
In compact form, we denote
$$x_{k+1} = Mx_k, k= 0,1,\ldots,$$
where 
$$x_k = \begin{pmatrix} 
h\\
a
\end{pmatrix}_{k}, M =  \begin{pmatrix} 
0 & B\\
B^T & 0
\end{pmatrix}$$
Because adjacency matrices are nonnegative by definition, the matrix $M$ is 
nonnegative too. $M$ is also clearly a symmetric matrix. Now we make a sequence 
of normalized vectors because only the relative scores do matter\footnote{$||.||_2$ is the Euclidean vector norm.}. 
\begin{eqnarray}\label{sequencezk}
  z_0 = x_0 > 0, z_{k+1} = \frac{Mz_k}{||Mz_k||_2}, k = 0,1,\ldots
\end{eqnarray}
We are now interested in the limit of the sequence $z_k$ and take this as the definition for
the hub and authority scores, but this would be a little bit problematic. The sequence doesn't always converge in the first place. We will prove that 
sequences associated with symmetric, nonnegative matrices $M$ oscillate between the 
limits:
$$z_{\text{even}} = \lim_{k \to \infty} z_{2k} \; \& \; z_{\text{odd}} = \lim_{k \to \infty} z_{2k+1}.$$
A second problem is that the limit vectors $z_{\text{even}}$ and $z_{\text{odd}}$ 
do in general depend on the initial vector $z_0$ and that there is no clear 
natural choice for $z_0$. The set of all limit vectors obtained when starting 
from any positive initial vector is given by:
$$Z = \{z_{\text{even}}(z_0), z_{\text{odd}}(z_0): z_0 > 0\},$$
and we have to select one vector in $Z$. A good choice is the vector $z_{\text{even}}$ 
obtained for $z_0 = \mathbf{1}$, because it has several nice properties that make it a good choice\footnote{$\mathbf{1}$ is a matrix, or vector, whose entries are all equal to 
$1$.}.
First, it is easy to compute and second, it has the extremal property of being 
the unique vector in $Z$ with the largest possible Manhattan norm\footnote{The 
$||.||_1$-norm or the Manhattan norm of a vector $\mathbf{x}$ is 
$||\mathbf{x}_1|| = \sum^n_{i=1} |x_i|.$}. This will also be proved later in this section.
Because of these properties, we take the subvectors of $z_{\text{even}}(\mathbf{1})$ 
as the definitions for the hub and authority scores. \\

Finally, notice that the second power of the matrix $M$ has the form:
$$M^2 = \begin{pmatrix} 
BB^T & 0\\
0 & B^TB
\end{pmatrix},$$
meaning that if the invariant subspaces associated with $BB^T$ and $B^TB$ have 
dimension 1, then the normalized hub and authority scores are given by the 
eigenvectors of $BB^T$ and $B^TB$. Also notice that when the invariant subspace 
has dimension 1, any positive vector $z_0$ would give the same result as 
starting vector $\mathbf{1}$.

\subsubsection{Developing a generalization of Kleinbergs method}
We introduce the concept of similarity on directed graphs by generalizing the 
construction of the previous paragraph. The authority score of vertex $v_j$ of a 
graph $\graf$ can be thought of as a similarity score between $v_j$ of $\graf$ 
and vertex \emph{authority} of the graph

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]


  \node[main node] (1) [label=left:\emph{hub}] {};
  \node[main node] (3) [label=right:\emph{authority}][right of=1] {};

  \path[every node/.style={font=\sffamily\small}]
   
    (1) edge node [left] {} (3)
  
    
\end{tikzpicture}
\end{center}
and, conversely, the hub score of vertex $v_j$ of $\graf$ can be seen as a similarity 
score between $v_j$ and vertex \emph{hub}. We call the hub-authority graph a 
\emph{structure graph} for this mutually reinforcing relation. Now the mutually reinforcing updating iteration used in 
the previous paragraph, can be generalized to graphs with other structure 
graphs. We start with an example. In our example, we use as structure graph a path graph with three vertices $v_1$, $v_2$, $v_3$. 

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]


  \node[main node] (1) [label=above:$v_1$] {};
  \node[main node] (2) [label=above:$v_2$][right of=1] {};
  \node[main node] (3) [label=above:$v_3$][right of=2] {};;

  \path[every node/.style={font=\sffamily\small}]
   
    (1) edge node [left] {} (2)
      (2) edge node [left] {} (3)
    
\end{tikzpicture}
\end{center}
Let $\graf(W,\to)$ be a graph. With each vertex $w_j$ of $\graf$ we now associate three scores $x_{i1}, x_{i2}$ and 
$x_{i3}$, one for each vertex of the structure graph. We initialize these score 
with a positive value and then update them according to the mutually reinforcing 
relation:
$$\begin{cases} x_{i1} := \hspace{75px} \sum_{j:(w_i,w_j)\in \to} x_{i2},\\ 
x_{i2} := \sum_{j:(w_j,w_i)\in \to} x_{i1}\hspace{75px} + \sum_{j:(w_i,w_j)\in \to} x_{i3},\\
x_{i3} := \hspace{75px}\sum_{j:(w_j,w_i)\in \to} x_{i2},
\end{cases}$$or, in matrix form ($\mathbf{x_j}$ denotes the column vector with entries $x_{ij}$),

$$\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}_{k+1} = \begin{pmatrix}
0 & B & 0\\
B^T & 0 & B\\
0 & B^T & 0
\end{pmatrix}\begin{pmatrix}
x_1\\
x_2\\
x_3
\end{pmatrix}_{k}   $$ 
which we again can denote by $x_{k+1} = Mx_k$. The example is now exactly the 
same as the previous example with hubs and authorities. The matrix $M$ is 
symmetric and nonnegative, the normalized even and odd iterates converge and the 
limit  $z_{\text{even}}(\mathbf{1})$  is, among all possible limits, the unique 
vector with the largest possbile Manhattan norm. The three components of the 
extremal limit  $z_{\text{even}}(\mathbf{1})$ are now defined as the \textit{similarity 
scores} $s_1, s_2, s_3$ and the \textit{similarity matrix} is defined by $\mathbf{S} = [s_1\; s_2\; s_3].$ 
We now give a numerical example.


\begin{example}
  Take as structure graph again the path graph with three vertices $v_1$, $v_2$, 
  $v_3$:

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]


  \node[main node] (1) [label=above:$v_1$] {};
  \node[main node] (2) [label=above:$v_2$][right of=1] {};
  \node[main node] (3) [label=above:$v_3$][right of=2] {};;

  \path[every node/.style={font=\sffamily\small}]
   
    (1) edge node [left] {} (2)
      (2) edge node [left] {} (3)
    
\end{tikzpicture}
\end{center}
Let $\graf(W,\to)$ be the following graph:
 \begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]

 \node[main node] (1) [label=above:$w_1$] {};

  \node[main node] (2) [label=left:$w_2$] [below left of =1; left of=3]{};
  \node[main node] (3) [label=right:$w_3$][below right of=1;] {};
    \node[main node] (4) [label=left:$w_4$][below of=2] {};
  \node[main node] (5) [label=right:$w_5$][below of=3] {};
  \path[every node/.style={font=\sffamily\small}]
       (1) edge node [left] {} (2)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (5)
            (2) edge node [left] {} (3)

      (2) edge node [left] {} (4)
      (3) edge node [left] {} (4)
      (3) edge node [left] {} (5)

\end{tikzpicture}
\end{center}
Then the adjacency matrix $B$ is:

$$B = \begin{pmatrix}
0 & 1 & 1 & 0 & 0\\
0 & 0 & 1 & 1 & 1\\
0 & 0 & 1 & 1 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
\end{pmatrix}$$
 By using the described mutually reinforcing updating iteration we become the 
 following similarity matrix (a numerical algorithm to calculate this is presented later 
 on in this section together with some proofs that facilitate the calculation):
 $$ S = \begin{pmatrix}
0.4433 & 0.1043 & 0 \\
0.2801 & 0.3956 & 0.0858 \\
0.0858 & 0.3956 & 0.2801\\
0.2216 & 0.0489 & 0.2216\\
0 & 0.1043 & 0.4433 \\
\end{pmatrix}$$
The similarity score of $w_4$ with $v_2$ of the structure graph is equal to 
$0.0489$.
 \end{example}

 \subsubsection{The general case}
 We now construct the general case. Take two directed graphs $\graf(U, \to)$ and $\grafeen(V, \to')$ 
 with $n_\graf$ and $n_\grafeen$ the order of the graphs. We think of $\graf$ as the structure 
 graph (such as the graphs $\text{hub}\to \text{authority}$ and the graph $1\to 2\to 
 3$ in the previous paragraphs). We consider the real scores $x_{ij}$ for $i = 1,\ldots,n_\grafeen$ and $j = 1,\ldots,n_\graf$ and we
 get the following mutually reinforcing updating iteration with the following updating equations:
 $$x_{ij} := \sum_{r:(v_r,v_i)\in \to', s:(u_s,u_j) \in \to} x_{rs} +  \sum_{r:(v_i,v_r)\in \to', s:(u_j,u_s) \in \to} x_{rs} $$
 Consider the product graph $\graf \times \grafeen.$ The above updating equation is equivalent to replacing
 the scores of all vertices of the product graph by the sum of the scores of the vertices linked by an incoming
 or outgoing edge. Equation \ref{sequencezk} can also be rewritten in a a more compact matrix form. Let $X_k$
 be the $n_\grafeen \times n_\graf$ matrix of entries $x_{ij}$ at iteration $k$, and $A$ and $B$ are the adjacency matrices
 of $\graf$ and $\grafeen$. Then the 
 updating equations can be written as:
 \begin{eqnarray}
X_{k+1} = BX_kA^T + B^TX_kA,\;\;\;\;\; k=0,1,\ldots,
  \end{eqnarray}
We'll prove that the normalized even and odd iterates of 
 this updating equation converge and that the limit $z_{\text{even}}(\mathbf{1})$ 
 is the limit with the largest Manhattan norm. This limit is the definition of 
 the similarity matrix. The following example shows a calculated similarity 
 matrix of two directed graphs.
 
 \begin{example}
   Let $\graf_A(V,\to)$ be the following graph:
 \begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]
  \node[main node] (2) [label=left:$v_2$] {};

 

  \node[main node] (1) [label=above:$v_1$][above right of=2] {};
      \node[main node] (3) [label=below:$v_3$][below right of = 2] {};
    \node[main node] (4) [label=right:$v_4$][below right of=1] {};


  \path[every node/.style={font=\sffamily\small}]
      (4) edge node [left] {} (1)
      (4) edge node [left] {} (3)
      (1) edge node [left] {} (3)
      (2) edge node [left] {} (3)
(3) edge node [left] {} (2)
(1) edge node [left] {} (2)
(2) edge node [left] {} (1)
\end{tikzpicture}
\end{center}
 Let $\graf_B(V',\to')$ be the following graph:
 \begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
                    \tikzstyle{every node}=[draw,circle,fill=black,minimum size=4pt,
                            inner sep=0pt]
  \node[main node] (6) [label=above left:$v'_6$] {};
   \node[main node] (1) [label=above:$v'_1$][above of=6] {};
   \node[main node] (2) [label=below:$v'_2$][below of=6] {};
   \node[main node] (4) [label=left:$v'_4$][left of=6] {};
   \node[main node] (3) [label=right:$v'_3$][right of=6] {};
   \node[main node] (5) [label=below:$v'_3$][below of=3] {};

  \path[every node/.style={font=\sffamily\small}]
      (1) edge node [left] {} (4)
      (1) edge node [left] {} (3)
       (2) edge node [left] {} (4)
      (2) edge node [left] {} (6)
      (3) edge node [left] {} (1)
      (3) edge node [left] {} (6)
      (3) edge node [left] {} (5)
      (6) edge node [left] {} (3)
      (6) edge node [left] {} (4)
      (6) edge node [left] {} (1)
\end{tikzpicture}
\end{center}
We become the following similarity matrix (a numerical algorithm to calculate this matrix is introduced later
in this section):
$$S = \begin{pmatrix}
0.2636 & 0.2786 & 0.2723 & 0.1289 \\
0.1286 & 0.1286 & 0.0624 & 0.1268 \\
0.2904 & 0.3115 & 0.2825 & 0.1667 \\
0.1540 & 0.1701 & 0.2462 & 0 \\
0.0634 & 0.0759 & 0.1018 & 0 \\
0.3038 & 0.3011 & 0.2532 & 0.1999\\
 \end{pmatrix}$$
 We see for example, that vertex $v_2$ of $\graf_A$ is most similar to
 vertex $v'_3$ in $\graf_B$  because the similarity score $s_{32}$ 
 is the highest among the similarity scores in $s_{2}$.
 \end{example}
 
 \subsection{Convergence of the sequence $z_k$}
In the introduction, we depend on the result that the sequence in Equation \ref{sequencezk} 
converges for even and odd iterates. We now investigate this hypothesis and 
prove a theorem about this convergence with the help of the Perron-Frobenius theory of the 
first chapter. Before we can start with this theorem, we first need some more 
corollaries of the Perron-Frobenius theorem for matrices that are not only nonnegative, but also symmetric.

\begin{theorem}
  Let $M$ be a symmetric nonnegative matrix with spectral radius $\rho$. Then 
  the algebraic and geometric multiplicity of the Perron root $\rho$ are equal; 
  there is a nonnegative matrix $X$ whose columns span the invariant subspace 
  associated with the Perron root; and the elements of the orthogonal projector 
  $\Pi$ on the vector space associated with the Perron root of $M$ are all 
  nonnegative.
\end{theorem}
\begin{proof}
  Ik heb nog enkele vragen over dit bewijs en heb de uitwerking hier daarom nog 
  even
  achterwege gelaten.
\end{proof}
 \begin{theorem}
   Let $M$ be a symmetric nonnegative matrix of spectral radius $\rho$. Let $z_0 > 0$ 
   and consider the sequence
   $$z_{k+1} = \frac{Mz_k}{||Mz_k||_2}, k = 0,1,\ldots$$
   Two convergence cases can occur depending on whether or not $-rho$ is an 
   eigenvalue of $M$. When $-\rho$ is not an eigenvalue of $M$, then the 
   sequence of $z_k$ simply converges to 
   $\frac{\Pi z_0}{||\Pi z_0||_2}$, where $\Pi$ is the orthogonal projector on the 
   invariant subspace associated with the Perron root $\rho$. When $-\rho$ is an 
   eigenvalue of $M$, then the subsequences $z_{2k}$ and $z_{2k+1}$ converge to 
   the limits
   $$z_{\text{even}}(z_0) = \lim_{k \to \infty} z_{2k}  = \frac{\Pi z_0}{||\Pi z_0||_2} \; \& \; z_{\text{odd}}(z_0) = \lim_{k \to \infty} z_{2k+1} = \frac{\Pi M z_0}{||\Pi M z_0||_2}.$$
and the vector $z_{\text{even}}(\mathbf{1})$ is the unique vector of largest 
possible Manhattan norm in that set. \end{theorem}

\begin{proof}
  Ik heb nog enkele vragen over dit bewijs en heb de uitwerking hier daarom nog 
  even
  achterwege gelaten.
\end{proof}

\subsection{Similarity matrices}


\subsection{Approximation algorithm}
Mag ik hier van de complexiteit van de klassieke power methode uitgaan als 
voorkennis? Is het opportuun mijn Pythoncode van dit algoritme hier af te 
drukken of is dit beter als bijlage of beter helemaal niet?
\subsection{Structure graphs}
Zwaar stuk. 
\subsection{Self-Similarity}
Gemakkelijk deel.
\subsection{Similarity of regular graphs}
Voor dit stuk moet de definitie van een reguliere directe graf nog toegevoegd 
worden aan het eerste deel. Voor de rest zeer gemakkelijk deel.
\subsection{Application to...}


   \newpage
\begin{thebibliography}{99}
\bibitem[EVES]{0} H. Eves, \emph{Elementary Matrix Theory}, Dover Publications, 
2012.
\bibitem[BAPAT] R. B. Bapat, T.E.. Raghavan, \emph{Nonnegative Matrices and 
Applications}, Encyclopedia of Mathematics and its Applications 64, Cambridge University Press, 1997. 
\bibitem[STERN2010]{1} S. Sternberg, \emph{The Perron-Frobenius theorem}, Chapter 9 in Dynamical Systems, Dover Publications, 2010.
\bibitem[NOUT2008]{2} D. Noutsos, \emph{Perron-Frobenius theory and some extensions} (lecture notes), Department of Mathematics, University of Ioannina, May 2008.
\bibitem[VARGA] R. S. Varga, \emph{Matrix Iterative Analysis}, Prentice-Hall, 1962. 
\bibitem R. A. Brualdi, H. J. Ryser, \emph{Combinatorial Matrix Theory}, Cambridge University 
Press, 1991
\bibitem P. Lancaster, M. Tismenetsky, \emph{The Theory of Matrices: With 
Applications}, Academic Press, 1985
\bibitem[CARA]{cara} P. Cara, \emph{Discrete Wiskunde}, Vrije Universiteit Brussel - Dienst 
Uitgaven, 2011
\bibitem[BLONDEL]{blondel} V. D Blondel, A. Gajardo, M. Heymans, P. Senellart, P. Van Dooren, 
\emph{A Measure of Similarity between Graph Vertices: Applications to Synonym Extraction and Web 
Searching}, Society for Industrial and Applied Mathematics, 2004.
\bibitem[KLEINBERG]{kleinberg} J. M. Kleinberg, \emph{Authoritative sources in a hyperlinked 
environment}, J. ACM, 1999.

\end{document}